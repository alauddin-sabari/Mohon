{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DoUpvote Please","metadata":{}},{"cell_type":"markdown","source":"## Load our dataset\n#### Display some images from our dataset\n#### Dataset contain images from seven different Categories","metadata":{}},{"cell_type":"code","source":"# display some images for every different expression\n\nimport numpy as np\nimport seaborn as sns\nfrom keras.preprocessing.image import load_img, img_to_array\nimport matplotlib.pyplot as plt\nimport os\n\n# size of the image: 48*48 pixels\npic_size = 48\n\n# input path for the images\nbase_path = \"../input/face-expression-recognition-dataset/images/\"\n\nplt.figure(0, figsize=(12,20))\ncpt = 0\n\nfor expression in os.listdir(base_path + \"train\"):\n    for i in range(1,6):\n        cpt = cpt + 1\n        plt.subplot(7,5,cpt)\n        img = load_img(base_path + \"train/\" + expression + \"/\" +os.listdir(base_path + \"train/\" + expression)[i], target_size=(pic_size, pic_size))\n        plt.imshow(img, cmap=\"gray\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image augmentation using keras ImageDataGenerator","metadata":{}},{"cell_type":"code","source":"# building data generator \n\nfrom keras.preprocessing.image import ImageDataGenerator\n\nbatch_size = 128\nbase_path = \"../input/face-expression-recognition-dataset/images/\"\n\n\ntrain_datagen = ImageDataGenerator(rescale = 1.0/255.0,\n                                  width_shift_range = 0.1,\n                                   height_shift_range = 0.1,\n                                   rotation_range = 20,\n                                   horizontal_flip = True)\n\nvalidation_datagen = ImageDataGenerator(rescale= 1.0/255)\n\ntrain_generator = train_datagen.flow_from_directory(base_path + \"train\",\n                                                    target_size=(56,56),\n                                                    color_mode=\"grayscale\",\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=True)\n\nvalidation_generator = validation_datagen.flow_from_directory(base_path + \"validation\",\n                                                    target_size=(56,56),\n                                                    color_mode=\"grayscale\",\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining our 4 Convolution and 2 Dense layers model","metadata":{}},{"cell_type":"code","source":"from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam\n\n# number of possible label values\nnb_classes = 7\n\n# Initialising the CNN\nmodel = Sequential()\n\n# 1 - Convolution\nmodel.add(Conv2D(64,(3,3), padding='same', input_shape=(56, 56,1)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 2nd Convolution layer\nmodel.add(Conv2D(128,(5,5), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 3rd Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 4th Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# Flattening\nmodel.add(Flatten())\n\n# Fully connected layer 1st layer\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n# Fully connected layer 2nd layer\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(nb_classes, activation='softmax'))\n\nprint(model.summary())\n\nopt = Adam(lr=0.0001)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# number of epochs to train the NN\nepochs = 50\n\n# checkpoint to save best model\nfrom keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n\nhistory = model.fit_generator(generator=train_generator,\n                                steps_per_epoch=train_generator.n//train_generator.batch_size,\n                                epochs=epochs,\n                                validation_data = validation_generator,\n                                validation_steps = validation_generator.n//validation_generator.batch_size,\n                                callbacks=callbacks_list\n                                )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualise training and testing accuracy and loss\n\ndef plot_results(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(1, len(acc) + 1)\n\n    plt.figure(figsize = (24, 6))\n    plt.subplot(1,2,1)\n    plt.plot(epochs, acc, 'b', label = 'Training Accuracy')\n    plt.plot(epochs, val_acc, 'r', label = 'Validation Accuracy')\n    plt.grid(True)\n    plt.legend()\n    plt.xlabel('Epoch')\n    \n\n\n    plt.subplot(1,2,2)\n    plt.plot(epochs, loss, 'b', label = 'Training Loss')\n    plt.plot(epochs, val_loss, 'r', label = 'Validation Loss')\n    plt.grid(True)\n    plt.legend()\n    plt.xlabel('Epoch')\n    plt.show()\n \n# print best epoch with best accuracy on validation\n\ndef get_best_epcoh(history):\n    valid_acc = history.history['val_accuracy']\n    best_epoch = valid_acc.index(max(valid_acc)) + 1\n    best_acc =  max(valid_acc)\n    print('Best Validation Accuracy Score {:0.5f}, is for epoch {}'.format( best_acc, best_epoch))\n    return best_epoch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_results(history)\nbest_epoch =get_best_epcoh(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Connecting with openCV","metadata":{"trusted":true}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nfrom tensorflow.keras.models import model_from_json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_json_file = '../input/face-expression-model/model.json'\nmodel_weights_file = '../input/face-expression-model/model_weights.h5'\nwith open(model_json_file, \"r\") as json_file:\n    loaded_model_json = json_file.read()\n    loaded_model = model_from_json(loaded_model_json)\n    loaded_model.load_weights(model_weights_file)\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"face_cascade = cv2.CascadeClassifier('../input/face-expression-model/haarcascade_frontalface_default.xml')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cap = cv2.VideoCapture(0)\nimport copy\n\nwhile True:\n    \n    ret, frame = cap.read()\n    img = copy.deepcopy(frame)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n    for (x,y,w,h) in faces:\n        fc = gray[y:y+h, x:x+w]\n        \n        roi = cv2.resize(fc, (48,48))\n        pred = loaded_model.predict(roi[np.newaxis, :, :, np.newaxis])\n        text_idx=np.argmax(pred)\n        text_list = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n        if text_idx == 0:\n            text= text_list[0]\n        if text_idx == 1:\n            text= text_list[1]\n        elif text_idx == 2:\n            text= text_list[2]\n        elif text_idx == 3:\n            text= text_list[3]\n        elif text_idx == 4:\n            text= text_list[4]\n        elif text_idx == 5:\n            text= text_list[5]\n        elif text_idx == 6:\n            text= text_list[6]\n        cv2.putText(img, text, (x, y-5),\n           cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 0, 255), 2)\n        img = cv2.rectangle(img, (x,y), (x+w, y+h), (0,0,255), 2)\n            \n    \n    cv2.imshow(\"frame\", img)\n    key = cv2.waitKey(1) & 0xFF\n    if key== ord('q'):\n        break\n    \ncap.release()\ncv2.destroyAllWindows()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}