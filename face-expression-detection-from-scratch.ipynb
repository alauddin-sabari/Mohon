{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DoUpvote Please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our dataset\n",
    "#### Display some images from our dataset\n",
    "#### Dataset contain images from seven different Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'kaggle kernels output nakulsingh1289/face-expression-detection-from-scratch -p /path/to/desttrain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(\u001b[38;5;241m0\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m20\u001b[39m))\n\u001b[1;32m     18\u001b[0m cpt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m expression \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m6\u001b[39m):\n\u001b[1;32m     22\u001b[0m         cpt \u001b[38;5;241m=\u001b[39m cpt \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'kaggle kernels output nakulsingh1289/face-expression-detection-from-scratch -p /path/to/desttrain'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display some images for every different expression\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.utils  import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# size of the image: 48*48 pixels\n",
    "pic_size = 48\n",
    "\n",
    "# input path for the images\n",
    "# base_path = \"./archive/images/\"\n",
    "base_path = \"kaggle kernels output nakulsingh1289/face-expression-detection-from-scratch -p /path/to/dest\"\n",
    "\n",
    "\n",
    "plt.figure(0, figsize=(12,20))\n",
    "cpt = 0\n",
    "\n",
    "for expression in os.listdir(base_path + \"train\"):\n",
    "    for i in range(1,6):\n",
    "        cpt = cpt + 1\n",
    "        plt.subplot(7,5,cpt)\n",
    "        img = load_img(base_path + \"train/\" + expression + \"/\" +os.listdir(base_path + \"train/\" + expression)[i], target_size=(pic_size, pic_size))\n",
    "        plt.imshow(img, cmap=\"gray\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image augmentation using keras ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building data generator \n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 128\n",
    "base_path = \"../input/face-expression-recognition-dataset/images/\"\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1.0/255.0,\n",
    "                                  width_shift_range = 0.1,\n",
    "                                   height_shift_range = 0.1,\n",
    "                                   rotation_range = 20,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale= 1.0/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(base_path + \"train\",\n",
    "                                                    target_size=(56,56),\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=True)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(base_path + \"validation\",\n",
    "                                                    target_size=(56,56),\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining our 4 Convolution and 2 Dense layers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# number of possible label values\n",
    "nb_classes = 7\n",
    "\n",
    "# Initialising the CNN\n",
    "model = Sequential()\n",
    "\n",
    "# 1 - Convolution\n",
    "model.add(Conv2D(64,(3,3), padding='same', input_shape=(56, 56,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 2nd Convolution layer\n",
    "model.add(Conv2D(128,(5,5), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 3rd Convolution layer\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 4th Convolution layer\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer 1st layer\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Fully connected layer 2nd layer\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "opt = Adam(lr=0.0001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# number of epochs to train the NN\n",
    "epochs = 50\n",
    "\n",
    "# checkpoint to save best model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                                steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
    "                                epochs=epochs,\n",
    "                                validation_data = validation_generator,\n",
    "                                validation_steps = validation_generator.n//validation_generator.batch_size,\n",
    "                                callbacks=callbacks_list\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise training and testing accuracy and loss\n",
    "\n",
    "def plot_results(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize = (24, 6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, acc, 'b', label = 'Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r', label = 'Validation Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    \n",
    "\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, loss, 'b', label = 'Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label = 'Validation Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()\n",
    " \n",
    "# print best epoch with best accuracy on validation\n",
    "\n",
    "def get_best_epcoh(history):\n",
    "    valid_acc = history.history['val_accuracy']\n",
    "    best_epoch = valid_acc.index(max(valid_acc)) + 1\n",
    "    best_acc =  max(valid_acc)\n",
    "    print('Best Validation Accuracy Score {:0.5f}, is for epoch {}'.format( best_acc, best_epoch))\n",
    "    return best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history)\n",
    "best_epoch =get_best_epcoh(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting with openCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json_file = '../input/face-expression-model/model.json'\n",
    "model_weights_file = '../input/face-expression-model/model_weights.h5'\n",
    "with open(model_json_file, \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(model_weights_file)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('../input/face-expression-model/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "import copy\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    img = copy.deepcopy(frame)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        fc = gray[y:y+h, x:x+w]\n",
    "        \n",
    "        roi = cv2.resize(fc, (48,48))\n",
    "        pred = loaded_model.predict(roi[np.newaxis, :, :, np.newaxis])\n",
    "        text_idx=np.argmax(pred)\n",
    "        text_list = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "        if text_idx == 0:\n",
    "            text= text_list[0]\n",
    "        if text_idx == 1:\n",
    "            text= text_list[1]\n",
    "        elif text_idx == 2:\n",
    "            text= text_list[2]\n",
    "        elif text_idx == 3:\n",
    "            text= text_list[3]\n",
    "        elif text_idx == 4:\n",
    "            text= text_list[4]\n",
    "        elif text_idx == 5:\n",
    "            text= text_list[5]\n",
    "        elif text_idx == 6:\n",
    "            text= text_list[6]\n",
    "        cv2.putText(img, text, (x, y-5),\n",
    "           cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 0, 255), 2)\n",
    "        img = cv2.rectangle(img, (x,y), (x+w, y+h), (0,0,255), 2)\n",
    "            \n",
    "    \n",
    "    cv2.imshow(\"frame\", img)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key== ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
